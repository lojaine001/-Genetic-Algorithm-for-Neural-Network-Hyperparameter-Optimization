"""
COMPLETELY REWRITTEN Neural Network Builder and Fitness Evaluator
This version GUARANTEES working fitness values
"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, optimizers
from typing import Dict, Any, Tuple
import time


class NeuralNetworkBuilder:
    """
    Builds and trains neural networks based on chromosome configuration
    """
    
    def __init__(self, input_shape: Tuple, num_classes: int):
        """
        Initialize builder
        
        Args:
            input_shape: Shape of input data (e.g., (28, 28) for MNIST)
            num_classes: Number of output classes
        """
        self.input_shape = input_shape
        self.num_classes = num_classes
        
        # For fitness normalization
        self.max_training_time = 120.0  # seconds
        self.max_model_complexity = 1e6  # parameters
    
    def build_model(self, chromosome: Dict[str, Any]) -> keras.Model:
        """
        Build a Keras model based on chromosome
        
        Args:
            chromosome: Hyperparameter configuration
            
        Returns:
            Compiled Keras model
        """
        model = models.Sequential()
        
        # Input layer - flatten if image data
        if len(self.input_shape) > 1:
            model.add(layers.Flatten(input_shape=self.input_shape))
        else:
            model.add(layers.InputLayer(input_shape=self.input_shape))
        
        # Hidden layers based on chromosome
        n_layers = chromosome['n_layers']
        layer_sizes = chromosome['layer_sizes']
        activation = chromosome['activation']
        dropout = chromosome['dropout']
        
        for i in range(n_layers):
            # Dense layer
            model.add(layers.Dense(
                layer_sizes[i],
                activation=activation,
                name=f'hidden_{i+1}'
            ))
            
            # Dropout for regularization
            if dropout > 0:
                model.add(layers.Dropout(dropout, name=f'dropout_{i+1}'))
        
        # Output layer
        model.add(layers.Dense(
            self.num_classes,
            activation='softmax',
            name='output'
        ))
        
        # Get optimizer
        optimizer = self._get_optimizer(
            chromosome['optimizer'],
            chromosome['learning_rate']
        )
        
        # Compile model
        model.compile(
            optimizer=optimizer,
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def _get_optimizer(self, optimizer_name: str, learning_rate: float):
        """
        Get Keras optimizer
        
        Args:
            optimizer_name: Name of optimizer
            learning_rate: Learning rate
            
        Returns:
            Keras optimizer
        """
        if optimizer_name == 'adam':
            return optimizers.Adam(learning_rate=learning_rate)
        elif optimizer_name == 'sgd':
            return optimizers.SGD(learning_rate=learning_rate, momentum=0.9)
        elif optimizer_name == 'rmsprop':
            return optimizers.RMSprop(learning_rate=learning_rate)
        else:
            return optimizers.Adam(learning_rate=learning_rate)
    
    def count_parameters(self, model: keras.Model) -> int:
        """
        Count total trainable parameters in model
        
        Args:
            model: Keras model
            
        Returns:
            Number of parameters
        """
        return int(np.sum([
            np.prod(v.get_shape().as_list()) 
            for v in model.trainable_weights
        ]))
    
    def evaluate_fitness(
        self,
        chromosome: Dict[str, Any],
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray,
        y_val: np.ndarray,
        epochs: int = 10,
        verbose: int = 0
    ) -> Dict[str, Any]:
        """
        REWRITTEN: Build, train, and evaluate model to compute fitness
        This version GUARANTEES non-zero results
        
        Args:
            chromosome: Hyperparameter configuration
            X_train: Training data
            y_train: Training labels
            X_val: Validation data
            y_val: Validation labels
            epochs: Number of training epochs
            verbose: Verbosity level
            
        Returns:
            Dictionary with fitness and metrics
        """
        print(f"\n  ðŸ”§ Building model: {chromosome['n_layers']} layers, "
              f"{chromosome['optimizer']}, lr={chromosome['learning_rate']:.6f}")
        
        try:
            # Build model
            model = self.build_model(chromosome)
            
            # Count parameters
            n_parameters = self.count_parameters(model)
            print(f"  ðŸ“Š Model parameters: {n_parameters:,}")
            
            # Train model and measure time
            start_time = time.time()
            
            # CRITICAL: Use fit without early stopping first
            history = model.fit(
                X_train, y_train,
                batch_size=chromosome['batch_size'],
                epochs=epochs,
                validation_data=(X_val, y_val),
                verbose=verbose
            )
            
            training_time = time.time() - start_time
            
            # CRITICAL: Get ACTUAL accuracy from history
            # This is where the bug was - we need to extract the right value
            train_acc = float(history.history['accuracy'][-1])
            val_acc = float(history.history['val_accuracy'][-1])
            
            print(f"  âœ… Training complete: Train Acc = {train_acc:.4f}, Val Acc = {val_acc:.4f}")
            print(f"  â±ï¸  Training time: {training_time:.2f}s")
            
            # Use validation accuracy for fitness
            accuracy = val_acc
            
            # SIMPLIFIED fitness calculation - just use accuracy primarily
            # Normalize metrics
            norm_accuracy = accuracy
            norm_time = max(0.0, 1.0 - (training_time / self.max_training_time))
            norm_complexity = max(0.0, 1.0 - (n_parameters / self.max_model_complexity))
            
            # Calculate fitness (weighted combination)
            # MORE WEIGHT ON ACCURACY
            fitness = (
                0.80 * norm_accuracy +       # 80% weight on accuracy
                0.15 * norm_time +            # 15% weight on speed
                0.05 * norm_complexity        # 5% weight on simplicity
            )
            
            # SAFETY CHECK: Ensure fitness is never negative or zero
            fitness = max(0.001, fitness)
            
            print(f"  ðŸŽ¯ FITNESS = {fitness:.4f} (acc={norm_accuracy:.4f}, "
                  f"time={norm_time:.4f}, complexity={norm_complexity:.4f})")
            
            # Clean up
            del model
            tf.keras.backend.clear_session()
            
            result = {
                'fitness': float(fitness),
                'accuracy': float(accuracy),
                'training_time': float(training_time),
                'n_parameters': int(n_parameters),
                'norm_accuracy': float(norm_accuracy),
                'norm_time': float(norm_time),
                'norm_complexity': float(norm_complexity),
                'train_accuracy': float(train_acc),
                'val_accuracy': float(val_acc)
            }
            
            # FINAL SAFETY CHECK
            assert result['fitness'] > 0, f"Fitness is zero! Result: {result}"
            
            return result
            
        except Exception as e:
            # If model fails, return LOW but NON-ZERO fitness
            print(f"  âš ï¸  Model training FAILED: {str(e)}")
            print(f"  âš ï¸  Returning low fitness (not zero)")
            
            return {
                'fitness': 0.001,  # CHANGED: Not zero, but very low
                'accuracy': 0.001,
                'training_time': 1.0,
                'n_parameters': 1000,
                'norm_accuracy': 0.001,
                'norm_time': 0.5,
                'norm_complexity': 0.5,
                'train_accuracy': 0.001,
                'val_accuracy': 0.001,
                'error': str(e)
            }


class BaselineComparator:
    """
    Compare GA results with baseline methods
    """
    
    def __init__(self, nn_builder: NeuralNetworkBuilder):
        """
        Initialize comparator
        
        Args:
            nn_builder: Neural network builder instance
        """
        self.nn_builder = nn_builder
    
    def random_search(
        self,
        n_trials: int,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray,
        y_val: np.ndarray,
        epochs: int = 10
    ) -> Dict[str, Any]:
        """
        Run random search baseline
        
        Args:
            n_trials: Number of random configurations to try
            X_train, y_train, X_val, y_val: Data
            epochs: Number of epochs to train
            
        Returns:
            Best configuration and results
        """
        print(f"\n{'='*60}")
        print(f"RANDOM SEARCH BASELINE ({n_trials} trials)")
        print(f"{'='*60}")
        
        from genetic_algorithm import GeneticAlgorithm
        
        ga = GeneticAlgorithm()  # Just to use search space
        
        best_fitness = -float('inf')
        best_config = None
        best_results = None
        all_results = []
        
        for i in range(n_trials):
            print(f"\nðŸŽ² Random Trial {i+1}/{n_trials}")
            
            # Generate random configuration
            config = ga.create_random_chromosome()
            
            # Evaluate
            results = self.nn_builder.evaluate_fitness(
                config, X_train, y_train, X_val, y_val, epochs=epochs
            )
            
            all_results.append({
                'config': config,
                'results': results
            })
            
            # Track best
            if results['fitness'] > best_fitness:
                best_fitness = results['fitness']
                best_config = config
                best_results = results
                print(f"  â­ NEW BEST!")
        
        print(f"\n{'='*60}")
        print(f"RANDOM SEARCH COMPLETE")
        print(f"  Best Fitness: {best_fitness:.4f}")
        print(f"  Best Accuracy: {best_results['accuracy']:.4f}")
        print(f"{'='*60}")
        
        return {
            'method': 'Random Search',
            'best_fitness': best_fitness,
            'best_config': best_config,
            'best_results': best_results,
            'all_results': all_results
        }
    
    def default_config(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray,
        y_val: np.ndarray,
        epochs: int = 10
    ) -> Dict[str, Any]:
        """
        Evaluate default/standard configuration
        
        Returns:
            Default configuration results
        """
        print(f"\n{'='*60}")
        print("DEFAULT CONFIGURATION BASELINE")
        print(f"{'='*60}")
        
        # Standard configuration
        config = {
            'n_layers': 2,
            'layer_sizes': [128, 64],
            'learning_rate': 0.001,
            'batch_size': 32,
            'dropout': 0.2,
            'optimizer': 'adam',
            'activation': 'relu'
        }
        
        print("\nðŸ“‹ Default config:")
        for key, value in config.items():
            print(f"  {key}: {value}")
        
        # Evaluate
        results = self.nn_builder.evaluate_fitness(
            config, X_train, y_train, X_val, y_val, epochs=epochs
        )
        
        print(f"\n{'='*60}")
        print("DEFAULT CONFIG COMPLETE")
        print(f"  Fitness: {results['fitness']:.4f}")
        print(f"  Accuracy: {results['accuracy']:.4f}")
        print(f"{'='*60}")
        
        return {
            'method': 'Default Config',
            'config': config,
            'results': results
        }
    
    def compare_all(
        self,
        ga_results: Dict[str, Any],
        random_results: Dict[str, Any],
        default_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Compare all methods
        
        Returns:
            Comparison summary
        """
        comparison = {
            'Genetic Algorithm': {
                'fitness': float(ga_results['best_fitness']),
                'accuracy': float(ga_results['best_results']['accuracy']),
                'training_time': float(ga_results['best_results']['training_time']),
                'n_parameters': int(ga_results['best_results']['n_parameters'])
            },
            'Random Search': {
                'fitness': float(random_results['best_fitness']),
                'accuracy': float(random_results['best_results']['accuracy']),
                'training_time': float(random_results['best_results']['training_time']),
                'n_parameters': int(random_results['best_results']['n_parameters'])
            },
            'Default Config': {
                'fitness': float(default_results['results']['fitness']),
                'accuracy': float(default_results['results']['accuracy']),
                'training_time': float(default_results['results']['training_time']),
                'n_parameters': int(default_results['results']['n_parameters'])
            }
        }
        
        # Print comparison table
        print("\n" + "="*80)
        print("COMPARISON OF METHODS")
        print("="*80)
        print(f"{'Method':<20} {'Fitness':>12} {'Accuracy':>12} {'Time (s)':>12} {'Parameters':>15}")
        print("-"*80)
        
        for method, metrics in comparison.items():
            print(f"{method:<20} "
                  f"{metrics['fitness']:>12.4f} "
                  f"{metrics['accuracy']:>12.4f} "
                  f"{metrics['training_time']:>12.2f} "
                  f"{metrics['n_parameters']:>15,}")
        
        print("="*80)
        
        # SAFETY CHECK
        assert comparison['Genetic Algorithm']['fitness'] > 0, "GA fitness is zero!"
        assert comparison['Random Search']['fitness'] > 0, "Random fitness is zero!"
        assert comparison['Default Config']['fitness'] > 0, "Default fitness is zero!"
        
        return comparison
